{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1635e341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataframe shape: (6010, 3)\n",
      "   Unnamed: 0                                        Description   Label\n",
      "0           0  ওই হালার পুত এখন কি মদ খাওয়ার সময় রাতের বেলা...  sexual\n",
      "1           1  আপনার জন্ম প্রক্রিয়ার সময় আপনার মায়ের ভিতর কি ...  sexual\n",
      "2           2  ধজভংগ দের আর ভায়াগ্রা লাগবো না। ধংস হোক এই সব ...  sexual\n",
      "3           3                                     বোকাচোদা একটা।  sexual\n",
      "4           4  তোর দেশে ফেরার অপেক্ষায় রইলাম। জেলে একটা কামরা...  sexual\n"
     ]
    }
   ],
   "source": [
    "# Imports, paths, and load raw data\n",
    "\n",
    "\n",
    "\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install --upgrade pip\n",
    "#!{sys.executable} -m pip install pandas\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Making sure project root is on path\n",
    "project_root = os.path.abspath(\"..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.config import DATA_DIR \n",
    "\n",
    "data_path = DATA_DIR / \"CyberBulling_Dataset_Bangla.xlsx\"\n",
    "df_raw = pd.read_excel(data_path)\n",
    "\n",
    "print(\"Raw dataframe shape:\", df_raw.shape)\n",
    "print(df_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42f39d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cleaning examples ===\n",
      "\n",
      "Original: ওই হালার পুত এখন কি মদ খাওয়ার সময় রাতের বেলা মদ খাই দিনের বেলাও মাঝেমধ্যে খায় এখন ম*** চ**** সময় safa কে একটু চুদাম যার ইচ্ছা আছে চুদার লাইনে দারা একজন একজন করে জাবি\n",
      "Cleaned : ওই হালার পুত এখন কি মদ খাওয়ার সময় রাতের বেলা মদ খাই দিনের বেলাও মাঝেমধ্যে খায় এখন ম*** চ**** সময় safa কে একটু চুদাম যার ইচ্ছা আছে চুদার লাইনে দারা একজন একজন করে জাবি\n",
      "\n",
      "Original: আপনার জন্ম প্রক্রিয়ার সময় আপনার মায়ের ভিতর কি আপনার বাবা হুমায়ুন কবিরের শুক্রাণু ঢুকে ছিল না অন্য কারো । তাইলে আপনি কেম্নে শিউর হইলেন উনিই আপনার বাবা। কারণ ওই কাজের সময়ে তো আপনি দেখেননি।আপনাকে জারজ বা জাউরা বললে কি ভুল হবে? পাক্নামি ছাইড়া দিয়া অভিনয়টা ভাল করে করেন।আমাগো আমজনতারে এইসব শুনাইয়েন না।\n",
      "Cleaned : আপনার জন্ম প্রক্রিয়ার সময় আপনার মায়ের ভিতর কি আপনার বাবা হুমায়ুন কবিরের শুক্রাণু ঢুকে ছিল না অন্য কারো । তাইলে আপনি কেম্নে শিউর হইলেন উনিই আপনার বাবা। কারণ ওই কাজের সময়ে তো আপনি দেখেননি।আপনাকে জারজ বা জাউরা বললে কি ভুল হবে? পাক্নামি ছাইড়া দিয়া অভিনয়টা ভাল করে করেন।আমাগো আমজনতারে এইসব শুনাইয়েন না।\n",
      "\n",
      "Original: ধজভংগ দের আর ভায়াগ্রা লাগবো না। ধংস হোক এই সব কুলখানকিরা।\n",
      "Cleaned : ধজভংগ দের আর ভায়াগ্রা লাগবো না। ধংস হোক এই সব কুলখানকিরা।\n",
      "\n",
      "Original: বোকাচোদা একটা।\n",
      "Cleaned : বোকাচোদা একটা।\n",
      "\n",
      "Original: তোর দেশে ফেরার অপেক্ষায় রইলাম। জেলে একটা কামরা বুক করে রাখা থাকল।\n",
      "Cleaned : তোর দেশে ফেরার অপেক্ষায় রইলাম। জেলে একটা কামরা বুক করে রাখা থাকল।\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "=== Cleaning examples (only changed rows) ===\n",
      "\n",
      "Original: নিজের রিভিউ বারানোর জন্যই জতো সব নাটক? ফালতু......\n",
      "Cleaned : নিজের রিভিউ বারানোর জন্যই জতো সব নাটক? ফালতু\n",
      "\n",
      "Original: আমরা বাকস্বাধীনতায় বিশ্বাসী৷ আপনার মতো পাঠকদের প্রশ্নের জবাব তিনি দিয়েছেন৷ পুরোটা পাবেন এই লিংকে: http://dw.com/p/1Gghk\n",
      "Cleaned : আমরা বাকস্বাধীনতায় বিশ্বাসী৷ আপনার মতো পাঠকদের প্রশ্নের জবাব তিনি দিয়েছেন৷ পুরোটা পাবেন এই লিংকে:\n",
      "\n",
      "Original: আমরা বাকস্বাধীনতায় বিশ্বাসী৷ আসিফের বক্তব্যের সঙ্গে আপনি একমত না হলে, আপনার অবস্থান জানাতে চাইলে লিখতে পারেন এই লিংকে, প্রতিবেদনের নিচে: http://dw.com/p/1Gghk\n",
      "Cleaned : আমরা বাকস্বাধীনতায় বিশ্বাসী৷ আসিফের বক্তব্যের সঙ্গে আপনি একমত না হলে, আপনার অবস্থান জানাতে চাইলে লিখতে পারেন এই লিংকে, প্রতিবেদনের নিচে:\n",
      "\n",
      "Original: প্রিয় পাঠক, আমরা বাকস্বাধীনতায় বিশ্বাসী৷ আসিফের বক্তব্যের সঙ্গে আপনি একমত না হলে, আপনার অবস্থান জানাতে চাইলে লিখতে পারেন এই লিংকে, প্রতিবেদনের নিচে: http://dw.com/p/1Gghk\n",
      "Cleaned : প্রিয় পাঠক, আমরা বাকস্বাধীনতায় বিশ্বাসী৷ আসিফের বক্তব্যের সঙ্গে আপনি একমত না হলে, আপনার অবস্থান জানাতে চাইলে লিখতে পারেন এই লিংকে, প্রতিবেদনের নিচে:\n",
      "\n",
      "Original: ব্রেকিং নিউজ, সবার জন্য দারুন খবর,http://bkas24.wapka.mobi ফেইসবুকের অলস সময়কে কাজে লাগান মাত্র ২ মিনিটে আয় করুন ৫০০ টাকা নাম মাত্র কিছু কাজ করেই আপনি পেতে পারেন ৫০০ টাকা মোবাইল রিচার্জ যেকোন সিমে। বিশ্বাস না হলেলিংকটিতে প্রবেশ করেই দেখুন এবং নির্দেশ মত কাজ করুন । আমি নিজে এইমাত্র ২০০ টাকা পেলাম । এটি পে-পাল ও ফেইসবুক অনুমোদিত।»» http://bkas24.wapka.mobi\n",
      "Cleaned : ব্রেকিং নিউজ, সবার জন্য দারুন খবর, ফেইসবুকের অলস সময়কে কাজে লাগান মাত্র ২ মিনিটে আয় করুন ৫০০ টাকা নাম মাত্র কিছু কাজ করেই আপনি পেতে পারেন ৫০০ টাকা মোবাইল রিচার্জ যেকোন সিমে। বিশ্বাস না হলেলিংকটিতে প্রবেশ করেই দেখুন এবং নির্দেশ মত কাজ করুন । আমি নিজে এইমাত্র ২০০ টাকা পেলাম । এটি পে-পাল ও ফেইসবুক অনুমোদিত।»»\n"
     ]
    }
   ],
   "source": [
    "#   Text cleaning helpers + quick test\n",
    "\n",
    "\n",
    "# Regex patterns\n",
    "URL_PATTERN = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "USERNAME_PATTERN = re.compile(r\"@\\w+\")\n",
    "MULTI_SPACE_PATTERN = re.compile(r\"\\s+\")\n",
    "\n",
    "# Decorative / noisy separator patterns:\n",
    "DECORATIVE_CHARS = \"-_=+|<>{}•●♦♠♥★☝✔⚫৤❣॥—০0∆\"\n",
    "SEPARATOR_PATTERN = re.compile(rf\"[{re.escape(DECORATIVE_CHARS)}]{{3,}}\")\n",
    "# Long repeats of the SAME non-alphanumeric character\n",
    "REPEAT_NOISE_PATTERN = re.compile(r\"([^A-Za-z0-9\\u0980-\\u09FF\\s])\\1{4,}\")\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Basic text cleaning for Bangla social media comments.\n",
    "\n",
    "    Design choices (based on EDA):\n",
    "    - Remove URLs (they are rarely informative for bullying).\n",
    "    - Remove @usernames (can leak personal info, not needed for label).\n",
    "    - Remove decorative separator patterns and long repeated symbols.\n",
    "    - Preserve emojis and punctuation because they carry\n",
    "      sentiment/emphasis that may help classification.\n",
    "    - Preserve numbers (e.g. years, counts) as they may appear in political content.\n",
    "    - Normalise whitespace.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    text = URL_PATTERN.sub(\" \", text)\n",
    "    text = USERNAME_PATTERN.sub(\" \", text)\n",
    "    text = SEPARATOR_PATTERN.sub(\" \", text)\n",
    "    text = REPEAT_NOISE_PATTERN.sub(\" \", text)\n",
    "    text = text.strip()\n",
    "    text = MULTI_SPACE_PATTERN.sub(\" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Quick test on a few examples\n",
    "print(\"=== Cleaning examples ===\")\n",
    "for i in range(5):\n",
    "    original = str(df_raw[\"Description\"].iloc[i])\n",
    "    cleaned = clean_text(original)\n",
    "    print(f\"\\nOriginal: {original}\")\n",
    "    print(f\"Cleaned : {cleaned}\")\n",
    "\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "print(\"=== Cleaning examples (only changed rows) ===\")\n",
    "\n",
    "df_raw[\"cleaned\"] = df_raw[\"Description\"].astype(str).apply(clean_text)\n",
    "changed = df_raw[df_raw[\"Description\"].astype(str) != df_raw[\"cleaned\"]]\n",
    "\n",
    "if changed.empty:\n",
    "    print(\"No rows changed by cleaning.\")\n",
    "else:\n",
    "    for _, row in changed.head(5).iterrows():\n",
    "        original = row[\"Description\"]\n",
    "        cleaned = row[\"cleaned\"]\n",
    "        print(f\"\\nOriginal: {original}\")\n",
    "        print(f\"Cleaned : {cleaned}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c4437b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Label encoding test (random 10) ===\n",
      "Sample labels: ['sexual', 'political', 'political', 'sexual', 'political', 'neutral', 'troll', 'sexual', 'political', 'neutral']\n",
      "Encoded     : [1 0 0 1 0 4 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "# Label mapping and encoding + test\n",
    "\n",
    "\n",
    "# Fixed mapping\n",
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"political\": 0,\n",
    "    \"sexual\": 1,\n",
    "    \"troll\": 2,\n",
    "    \"threat\": 3,\n",
    "    \"neutral\": 4,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "\n",
    "def encode_labels(labels: pd.Series) -> np.ndarray:\n",
    "    \"\"\"Map string labels to numeric ids.\"\"\"\n",
    "    labels = labels.astype(str).str.lower()\n",
    "    encoded = labels.map(LABEL2ID).values\n",
    "    return encoded\n",
    "\n",
    "\n",
    "# Quick test on first few labels\n",
    "print(\"=== Label encoding test (random 10) ===\")\n",
    "sample_labels = df_raw[\"Label\"].sample(n=10, random_state=42).astype(str).str.lower().reset_index(drop=True)\n",
    "print(\"Sample labels:\", sample_labels.tolist())\n",
    "print(\"Encoded     :\", encode_labels(sample_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad823056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared dataframe shape: (5858, 2)\n",
      "                                         Description   Label\n",
      "0  ওই হালার পুত এখন কি মদ খাওয়ার সময় রাতের বেলা...  sexual\n",
      "1  আপনার জন্ম প্রক্রিয়ার সময় আপনার মায়ের ভিতর কি ...  sexual\n",
      "2  ধজভংগ দের আর ভায়াগ্রা লাগবো না। ধংস হোক এই সব ...  sexual\n",
      "3                                     বোকাচোদা একটা।  sexual\n",
      "4  তোর দেশে ফেরার অপেক্ষায় রইলাম। জেলে একটা কামরা...  sexual\n",
      "\n",
      "Label distribution:\n",
      "Label\n",
      "neutral      1199\n",
      "troll        1197\n",
      "sexual       1195\n",
      "threat       1191\n",
      "political    1076\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y_/915f7xdj6rj1jcxp6zd5k9lw0000gn/T/ipykernel_32757/909200331.py:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[text_col].replace(\"\", np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# prepare_dataframe() definition + run\n",
    "\n",
    "\n",
    "def prepare_dataframe(\n",
    "    df: pd.DataFrame,\n",
    "    text_col: str = \"Description\",\n",
    "    label_col: str = \"Label\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Select relevant columns, clean text, drop nulls and duplicates.\n",
    "    \"\"\"\n",
    "    # Select relevant columns\n",
    "    df = df[[text_col, label_col]].copy()\n",
    "\n",
    "    # Normalise labels to lower-case strings\n",
    "    df[label_col] = df[label_col].astype(str).str.lower()\n",
    "\n",
    "    # Text cleaning\n",
    "    df[text_col] = df[text_col].astype(str).apply(clean_text)\n",
    "\n",
    "    # Drop rows with empty text or null labels\n",
    "    df[text_col].replace(\"\", np.nan, inplace=True)\n",
    "    df.dropna(subset=[text_col, label_col], inplace=True)\n",
    "\n",
    "    # Drop exact duplicate entries (text + label)\n",
    "    df.drop_duplicates(subset=[text_col, label_col], inplace=True)\n",
    "\n",
    "    # Reset index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = prepare_dataframe(df_raw, text_col=\"Description\", label_col=\"Label\")\n",
    "\n",
    "print(\"Prepared dataframe shape:\", df.shape)\n",
    "print(df.head())\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "719ce016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4100\n",
      "Val size  : 879\n",
      "Test size : 879\n",
      "\n",
      "Train label distribution:\n",
      "0    753\n",
      "1    836\n",
      "2    838\n",
      "3    834\n",
      "4    839\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Val label distribution:\n",
      "0    161\n",
      "1    180\n",
      "2    179\n",
      "3    179\n",
      "4    180\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test label distribution:\n",
      "0    162\n",
      "1    179\n",
      "2    180\n",
      "3    178\n",
      "4    180\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# stratified_splits() definition + run\n",
    "\n",
    "\n",
    "def stratified_splits(\n",
    "    df: pd.DataFrame,\n",
    "    text_col: str = \"Description\",\n",
    "    label_col: str = \"Label\",\n",
    "    train_size: float = 0.70,\n",
    "    val_size: float = 0.15,\n",
    "    test_size: float = 0.15,\n",
    "    random_state: int = 42,\n",
    ") -> Tuple[pd.Series, pd.Series, pd.Series, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create stratified train/val/test splits.\n",
    "\n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    assert np.isclose(train_size + val_size + test_size, 1.0), \\\n",
    "        \"train_size + val_size + test_size must equal 1.0\"\n",
    "\n",
    "    # Encode labels to ids\n",
    "    y_all = encode_labels(df[label_col])\n",
    "    X_all = df[text_col].values\n",
    "\n",
    "    # First split: train vs temp\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_all,\n",
    "        y_all,\n",
    "        test_size=(1.0 - train_size),\n",
    "        stratify=y_all,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    # Second split: val vs test from temp\n",
    "    relative_test_size = test_size / (test_size + val_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp,\n",
    "        y_temp,\n",
    "        test_size=relative_test_size,\n",
    "        stratify=y_temp,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = stratified_splits(df)\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Val size  :\", len(X_val))\n",
    "print(\"Test size :\", len(X_test))\n",
    "\n",
    "# Check label balance in each split\n",
    "print(\"\\nTrain label distribution:\")\n",
    "print(pd.Series(y_train).value_counts().sort_index())\n",
    "print(\"\\nVal label distribution:\")\n",
    "print(pd.Series(y_val).value_counts().sort_index())\n",
    "print(\"\\nTest label distribution:\")\n",
    "print(pd.Series(y_test).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebb9b6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded train shape: (4100, 128)\n",
      "Padded val shape  : (879, 128)\n",
      "Padded test shape : (879, 128)\n",
      "\n",
      "Example text: আপনি অভিনয় জগতের মানুষ। অভিনয় করতে গিয়ে অনেক সময় শরিয়ত লঙ্ঘন হয়। এতে আপনি ফাসিক হবেন কাফির না। কাফির অপেক্ষা ফাসিক উত্তম। কিন্তু ফাসিকের জন্যও পরকালে রয়েছে ভয়ঙ্কর শাস্তি। এজন্যই হয়তো সেদিন লাইভে লোকটা আপনাকে পরকালে বিশ্বাস আছে কি না জিজ্ঞাস করে। আপনি চাইলে প্রশ্নটা ইগনোর করতে পারতেন। উপস্থাপকো আপনাকে তাই করতে বলেছিল। কিন্তু, আপনি নিজের ক্যারিয়ারটাকে ডিফেন্স করতে গিয়ে এমন একটা কথা বললেন যা রাতারাতি আপনাকে কুখ্যাত করে দিল। ইসলামে যে সাতটি বিষয়ের উপর ইমান আনা মুসলমানের উপর ফরজ পরকাল তার মধ্যে অন্যতম। পরকালকে অস্বীকার করে আপনি নিজেকে মুসলমান হিসেবে পরিচয় দিতে পারেন না। জানিনা এখন কোন উদ্দেশ্যে আবার আল্লাহর দোহাই দিচ্ছেন। যদি ক্যারিয়ারের ভয়ে ক্ষমা চেয়ে থাকেন তাহলে বলব আপনার যা ক্ষতি হওয়ার হয়ে গেছে। আর যদি সত্যি অনুতপ্ত হয়ে থাকেন তাহলে বলব, আসলে আপনি যে ভূল করেছেন তা আল্লাহর সাথেই করেছেন। কোন মানুষের সাথে নয়। তাই ক্ষমা যদি চাইতেই হয় তবে তওবা করে আল্লাহর কাছে ক্ষমা চান। ইন্নাল্লাহা গাফুরুর রাহীম।\n",
      "Example sequence: [  16  332 3703 1151  332   34  426   61  142 5849 5850  272  610   16\n",
      " 2220 1038 1806   38 1806 1540 2220 5851   56 5852 3704  169 1807 3705\n",
      " 5853 1808]\n"
     ]
    }
   ],
   "source": [
    "# Keras tokenizer + padded sequences\n",
    "\n",
    "\n",
    "def build_keras_tokenizer(\n",
    "    texts: List[str],\n",
    "    num_words: int = None,\n",
    "    oov_token: str = \"[OOV]\"\n",
    ") -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Fit a Keras Tokenizer on the training texts.\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def texts_to_padded_sequences(\n",
    "    tokenizer: Tokenizer,\n",
    "    texts: List[str],\n",
    "    max_len: int = 128\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a list/array of texts into padded sequences.\n",
    "    \"\"\"\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "    return padded\n",
    "\n",
    "# Max length setted based on EDA ( Max ≈ 210 words, 95th percentile ≈ ~57 words)\n",
    "max_len = 128\n",
    "tokenizer = build_keras_tokenizer(X_train)\n",
    "\n",
    "X_train_pad = texts_to_padded_sequences(tokenizer, X_train, max_len=max_len)\n",
    "X_val_pad = texts_to_padded_sequences(tokenizer, X_val, max_len=max_len)\n",
    "X_test_pad = texts_to_padded_sequences(tokenizer, X_test, max_len=max_len)\n",
    "\n",
    "print(\"Padded train shape:\", X_train_pad.shape)\n",
    "print(\"Padded val shape  :\", X_val_pad.shape)\n",
    "print(\"Padded test shape :\", X_test_pad.shape)\n",
    "\n",
    "# Example\n",
    "idx = 0\n",
    "print(\"\\nExample text:\", X_train[idx])\n",
    "print(\"Example sequence:\", X_train_pad[idx][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dde80a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input_ids shape    : (8, 128)\n",
      "Sample attention_mask shape: (8, 128)\n",
      "\n",
      "First row input_ids (first 20 tokens):\n",
      "[     0  36827 232388 216907  86260  58041    125 232388  12173 103294\n",
      "  34694  20895  17145  20824   2801   3397  48319  76675   7212  38477]\n"
     ]
    }
   ],
   "source": [
    "# Transformer tokenizer helpers + test\n",
    "\n",
    "\n",
    "def load_transformer_tokenizer(model_name: str = \"xlm-roberta-base\"):\n",
    "    \"\"\"\n",
    "    Load a pretrained Hugging Face tokenizer.\n",
    "\n",
    "    Based on EDA (Bangla + code-mixing), a multilingual subword model such as\n",
    "    'xlm-roberta-base' or 'bert-base-multilingual-cased' is appropriate.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def encode_texts_transformer(\n",
    "    tokenizer,\n",
    "    texts: List[str],\n",
    "    max_len: int = 128\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Encode texts using a Hugging Face tokenizer.\n",
    "\n",
    "    Returns numpy arrays:\n",
    "        {\n",
    "            \"input_ids\": shape (N, max_len),\n",
    "            \"attention_mask\": shape (N, max_len)\n",
    "        }\n",
    "    \"\"\"\n",
    "    encodings = tokenizer(\n",
    "        list(texts),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    input_ids = np.array(encodings[\"input_ids\"], dtype=np.int64)\n",
    "    attention_mask = np.array(encodings[\"attention_mask\"], dtype=np.int64)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "\n",
    "# Load tokenizer and encode a small subset to test\n",
    "hf_model_name = \"xlm-roberta-base\"\n",
    "hf_tokenizer = load_transformer_tokenizer(hf_model_name)\n",
    "\n",
    "sample_texts = list(X_train[:8])\n",
    "enc_sample = encode_texts_transformer(hf_tokenizer, sample_texts, max_len=128)\n",
    "\n",
    "print(\"Sample input_ids shape    :\", enc_sample[\"input_ids\"].shape)\n",
    "print(\"Sample attention_mask shape:\", enc_sample[\"attention_mask\"].shape)\n",
    "print(\"\\nFirst row input_ids (first 20 tokens):\")\n",
    "print(enc_sample[\"input_ids\"][0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e65e96fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary ===\n",
      "Final prepared df shape: (5858, 2)\n",
      "Train/Val/Test sizes: 4100 879 879\n",
      "Keras padded train shape: (4100, 128)\n",
      "Transformer sample Enc shape: (8, 128)\n"
     ]
    }
   ],
   "source": [
    "# wrap up / summary print\n",
    "\n",
    "\n",
    "print(\"=== Summary ===\")\n",
    "print(\"Final prepared df shape:\", df.shape)\n",
    "print(\"Train/Val/Test sizes:\", len(X_train), len(X_val), len(X_test))\n",
    "print(\"Keras padded train shape:\", X_train_pad.shape)\n",
    "print(\"Transformer sample Enc shape:\", enc_sample[\"input_ids\"].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
