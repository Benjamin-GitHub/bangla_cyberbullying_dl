{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1635e341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataframe shape: (6010, 3)\n",
      "   Unnamed: 0                                        Description   Label\n",
      "0           0  ওই হালার পুত এখন কি মদ খাওয়ার সময় রাতের বেলা...  sexual\n",
      "1           1  আপনার জন্ম প্রক্রিয়ার সময় আপনার মায়ের ভিতর কি ...  sexual\n",
      "2           2  ধজভংগ দের আর ভায়াগ্রা লাগবো না। ধংস হোক এই সব ...  sexual\n",
      "3           3                                     বোকাচোদা একটা।  sexual\n",
      "4           4  তোর দেশে ফেরার অপেক্ষায় রইলাম। জেলে একটা কামরা...  sexual\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Imports, paths, and load raw data\n",
    "# ============================================\n",
    "\n",
    "\n",
    "#import sys\n",
    "#!{sys.executable} -m pip install --upgrade pip\n",
    "#!{sys.executable} -m pip install pandas\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Making sure project root is on path\n",
    "project_root = os.path.abspath(\"..\")\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.config import DATA_DIR \n",
    "\n",
    "data_path = DATA_DIR / \"CyberBulling_Dataset_Bangla.xlsx\"\n",
    "df_raw = pd.read_excel(data_path)\n",
    "\n",
    "print(\"Raw dataframe shape:\", df_raw.shape)\n",
    "print(df_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f39d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Cleaning examples ===\n",
      "\n",
      "Original: ওই হালার পুত এখন কি মদ খাওয়ার সময় রাতের বেলা মদ খাই দিনের বেলাও মাঝেমধ্যে খায় এখন ম*** চ**** সময় safa কে একটু চুদাম যার ইচ্ছা আছে চুদার লাইনে দারা একজন একজন করে জাবি\n",
      "Cleaned : ওই হালার পুত এখন কি মদ খাওয়ার সময় রাতের বেলা মদ খাই দিনের বেলাও মাঝেমধ্যে খায় এখন ম*** চ**** সময় safa কে একটু চুদাম যার ইচ্ছা আছে চুদার লাইনে দারা একজন একজন করে জাবি\n",
      "\n",
      "Original: আপনার জন্ম প্রক্রিয়ার সময় আপনার মায়ের ভিতর কি আপনার বাবা হুমায়ুন কবিরের শুক্রাণু ঢুকে ছিল না অন্য কারো । তাইলে আপনি কেম্নে শিউর হইলেন উনিই আপনার বাবা। কারণ ওই কাজের সময়ে তো আপনি দেখেননি।আপনাকে জারজ বা জাউরা বললে কি ভুল হবে? পাক্নামি ছাইড়া দিয়া অভিনয়টা ভাল করে করেন।আমাগো আমজনতারে এইসব শুনাইয়েন না।\n",
      "Cleaned : আপনার জন্ম প্রক্রিয়ার সময় আপনার মায়ের ভিতর কি আপনার বাবা হুমায়ুন কবিরের শুক্রাণু ঢুকে ছিল না অন্য কারো । তাইলে আপনি কেম্নে শিউর হইলেন উনিই আপনার বাবা। কারণ ওই কাজের সময়ে তো আপনি দেখেননি।আপনাকে জারজ বা জাউরা বললে কি ভুল হবে? পাক্নামি ছাইড়া দিয়া অভিনয়টা ভাল করে করেন।আমাগো আমজনতারে এইসব শুনাইয়েন না।\n",
      "\n",
      "Original: ধজভংগ দের আর ভায়াগ্রা লাগবো না। ধংস হোক এই সব কুলখানকিরা।\n",
      "Cleaned : ধজভংগ দের আর ভায়াগ্রা লাগবো না। ধংস হোক এই সব কুলখানকিরা।\n",
      "\n",
      "Original: বোকাচোদা একটা।\n",
      "Cleaned : বোকাচোদা একটা।\n",
      "\n",
      "Original: তোর দেশে ফেরার অপেক্ষায় রইলাম। জেলে একটা কামরা বুক করে রাখা থাকল।\n",
      "Cleaned : তোর দেশে ফেরার অপেক্ষায় রইলাম। জেলে একটা কামরা বুক করে রাখা থাকল।\n",
      "-----------------------------------------------------------------\n",
      "-----------------------------------------------------------------\n",
      "=== Cleaning examples (only changed rows) ===\n",
      "\n",
      "Original: নিজের রিভিউ বারানোর জন্যই জতো সব নাটক? ফালতু......\n",
      "Cleaned : নিজের রিভিউ বারানোর জন্যই জতো সব নাটক? ফালতু\n",
      "\n",
      "Original: আমরা বাকস্বাধীনতায় বিশ্বাসী৷ আপনার মতো পাঠকদের প্রশ্নের জবাব তিনি দিয়েছেন৷ পুরোটা পাবেন এই লিংকে: http://dw.com/p/1Gghk\n",
      "Cleaned : আমরা বাকস্বাধীনতায় বিশ্বাসী৷ আপনার মতো পাঠকদের প্রশ্নের জবাব তিনি দিয়েছেন৷ পুরোটা পাবেন এই লিংকে:\n",
      "\n",
      "Original: আমরা বাকস্বাধীনতায় বিশ্বাসী৷ আসিফের বক্তব্যের সঙ্গে আপনি একমত না হলে, আপনার অবস্থান জানাতে চাইলে লিখতে পারেন এই লিংকে, প্রতিবেদনের নিচে: http://dw.com/p/1Gghk\n",
      "Cleaned : আমরা বাকস্বাধীনতায় বিশ্বাসী৷ আসিফের বক্তব্যের সঙ্গে আপনি একমত না হলে, আপনার অবস্থান জানাতে চাইলে লিখতে পারেন এই লিংকে, প্রতিবেদনের নিচে:\n",
      "\n",
      "Original: প্রিয় পাঠক, আমরা বাকস্বাধীনতায় বিশ্বাসী৷ আসিফের বক্তব্যের সঙ্গে আপনি একমত না হলে, আপনার অবস্থান জানাতে চাইলে লিখতে পারেন এই লিংকে, প্রতিবেদনের নিচে: http://dw.com/p/1Gghk\n",
      "Cleaned : প্রিয় পাঠক, আমরা বাকস্বাধীনতায় বিশ্বাসী৷ আসিফের বক্তব্যের সঙ্গে আপনি একমত না হলে, আপনার অবস্থান জানাতে চাইলে লিখতে পারেন এই লিংকে, প্রতিবেদনের নিচে:\n",
      "\n",
      "Original: ব্রেকিং নিউজ, সবার জন্য দারুন খবর,http://bkas24.wapka.mobi ফেইসবুকের অলস সময়কে কাজে লাগান মাত্র ২ মিনিটে আয় করুন ৫০০ টাকা নাম মাত্র কিছু কাজ করেই আপনি পেতে পারেন ৫০০ টাকা মোবাইল রিচার্জ যেকোন সিমে। বিশ্বাস না হলেলিংকটিতে প্রবেশ করেই দেখুন এবং নির্দেশ মত কাজ করুন । আমি নিজে এইমাত্র ২০০ টাকা পেলাম । এটি পে-পাল ও ফেইসবুক অনুমোদিত।»» http://bkas24.wapka.mobi\n",
      "Cleaned : ব্রেকিং নিউজ, সবার জন্য দারুন খবর, ফেইসবুকের অলস সময়কে কাজে লাগান মাত্র ২ মিনিটে আয় করুন ৫০০ টাকা নাম মাত্র কিছু কাজ করেই আপনি পেতে পারেন ৫০০ টাকা মোবাইল রিচার্জ যেকোন সিমে। বিশ্বাস না হলেলিংকটিতে প্রবেশ করেই দেখুন এবং নির্দেশ মত কাজ করুন । আমি নিজে এইমাত্র ২০০ টাকা পেলাম । এটি পে-পাল ও ফেইসবুক অনুমোদিত।»»\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "#   Text cleaning helpers + quick test\n",
    "# ============================================\n",
    "\n",
    "# Regex patterns\n",
    "URL_PATTERN = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "USERNAME_PATTERN = re.compile(r\"@\\w+\")\n",
    "MULTI_SPACE_PATTERN = re.compile(r\"\\s+\")\n",
    "\n",
    "# Decorative / noisy separator patterns:\n",
    "DECORATIVE_CHARS = \"-_=+|<>{}•●♦♠♥★☝✔⚫৤❣॥—০0∆\"\n",
    "SEPARATOR_PATTERN = re.compile(rf\"[{re.escape(DECORATIVE_CHARS)}]{{3,}}\")\n",
    "# Long repeats of the SAME non-alphanumeric character\n",
    "REPEAT_NOISE_PATTERN = re.compile(r\"([^A-Za-z0-9\\u0980-\\u09FF\\s])\\1{4,}\")\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Basic text cleaning for Bangla social media comments.\n",
    "\n",
    "    Design choices (based on EDA):\n",
    "    - Remove URLs (they are rarely informative for bullying).\n",
    "    - Remove @usernames (can leak personal info, not needed for label).\n",
    "    - Remove decorative separator patterns and long repeated symbols.\n",
    "    - Preserve emojis and punctuation because they carry\n",
    "      sentiment/emphasis that may help classification.\n",
    "    - Preserve numbers (e.g. years, counts) as they may appear in political content.\n",
    "    - Normalise whitespace.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "\n",
    "    text = URL_PATTERN.sub(\" \", text)\n",
    "    text = USERNAME_PATTERN.sub(\" \", text)\n",
    "    text = SEPARATOR_PATTERN.sub(\" \", text)\n",
    "    text = REPEAT_NOISE_PATTERN.sub(\" \", text)\n",
    "    text = text.strip()\n",
    "    text = MULTI_SPACE_PATTERN.sub(\" \", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Quick test on a few examples\n",
    "print(\"=== Cleaning examples ===\")\n",
    "for i in range(5):\n",
    "    original = str(df_raw[\"Description\"].iloc[i])\n",
    "    cleaned = clean_text(original)\n",
    "    print(f\"\\nOriginal: {original}\")\n",
    "    print(f\"Cleaned : {cleaned}\")\n",
    "\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "print(\"-----------------------------------------------------------------\")\n",
    "\n",
    "print(\"=== Cleaning examples (only changed rows) ===\")\n",
    "\n",
    "df_raw[\"cleaned\"] = df_raw[\"Description\"].astype(str).apply(clean_text)\n",
    "changed = df_raw[df_raw[\"Description\"].astype(str) != df_raw[\"cleaned\"]]\n",
    "\n",
    "if changed.empty:\n",
    "    print(\"No rows changed by cleaning.\")\n",
    "else:\n",
    "    for _, row in changed.head(5).iterrows():\n",
    "        original = row[\"Description\"]\n",
    "        cleaned = row[\"cleaned\"]\n",
    "        print(f\"\\nOriginal: {original}\")\n",
    "        print(f\"Cleaned : {cleaned}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4437b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Label encoding test ===\n",
      "Sample labels: ['sexual', 'sexual', 'sexual', 'sexual', 'sexual', 'sexual', 'sexual', 'sexual', 'sexual', 'sexual']\n",
      "Encoded     : [1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Label mapping and encoding + test\n",
    "# ============================================\n",
    "\n",
    "# Fixed mapping as requested\n",
    "LABEL2ID: Dict[str, int] = {\n",
    "    \"political\": 0,\n",
    "    \"sexual\": 1,\n",
    "    \"troll\": 2,\n",
    "    \"threat\": 3,\n",
    "    \"neutral\": 4,\n",
    "}\n",
    "ID2LABEL: Dict[int, str] = {v: k for k, v in LABEL2ID.items()}\n",
    "\n",
    "\n",
    "def encode_labels(labels: pd.Series) -> np.ndarray:\n",
    "    \"\"\"Map string labels to numeric ids.\"\"\"\n",
    "    labels = labels.astype(str).str.lower()\n",
    "    encoded = labels.map(LABEL2ID).values\n",
    "    return encoded\n",
    "\n",
    "\n",
    "# Quick test on first few labels\n",
    "print(\"=== Label encoding test ===\")\n",
    "sample_labels = df_raw[\"Label\"].head(10).astype(str).str.lower()\n",
    "print(\"Sample labels:\", sample_labels.tolist())\n",
    "print(\"Encoded     :\", encode_labels(sample_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad823056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared dataframe shape: (5860, 2)\n",
      "                                         Description   Label\n",
      "0  ওই হালার পুত এখন কি মদ খাওয়ার সময় রাতের বেলা...  sexual\n",
      "1  আপনার জন্ম প্রক্রিয়ার সময় আপনার মায়ের ভিতর কি ...  sexual\n",
      "2  ধজভংগ দের আর ভায়াগ্রা লাগবো না। ধংস হোক এই সব ...  sexual\n",
      "3                                     বোকাচোদা একটা।  sexual\n",
      "4  তোর দেশে ফেরার অপেক্ষায় রইলাম। জেলে একটা কামরা...  sexual\n",
      "\n",
      "Label distribution:\n",
      "Label\n",
      "neutral      1200\n",
      "troll        1197\n",
      "sexual       1195\n",
      "threat       1192\n",
      "political    1076\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y_/915f7xdj6rj1jcxp6zd5k9lw0000gn/T/ipykernel_21836/2469638249.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[text_col].replace(\"\", np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# prepare_dataframe() definition + run\n",
    "# ============================================\n",
    "\n",
    "def prepare_dataframe(\n",
    "    df: pd.DataFrame,\n",
    "    text_col: str = \"Description\",\n",
    "    label_col: str = \"Label\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Select relevant columns, clean text, drop empty/nulls and duplicates,\n",
    "    and ensure labels are in canonical form.\n",
    "    \"\"\"\n",
    "    # Select relevant columns\n",
    "    df = df[[text_col, label_col]].copy()\n",
    "\n",
    "    # Normalise labels to lower-case strings\n",
    "    df[label_col] = df[label_col].astype(str).str.lower()\n",
    "\n",
    "    # Text cleaning\n",
    "    df[text_col] = df[text_col].astype(str).apply(clean_text)\n",
    "\n",
    "    # Drop rows with empty text or null labels\n",
    "    df[text_col].replace(\"\", np.nan, inplace=True)\n",
    "    df.dropna(subset=[text_col, label_col], inplace=True)\n",
    "\n",
    "    # Drop exact duplicate entries (text + label)\n",
    "    df.drop_duplicates(subset=[text_col, label_col], inplace=True)\n",
    "\n",
    "    # Reset index\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = prepare_dataframe(df_raw, text_col=\"Description\", label_col=\"Label\")\n",
    "\n",
    "print(\"Prepared dataframe shape:\", df.shape)\n",
    "print(df.head())\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df[\"Label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "719ce016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4101\n",
      "Val size  : 879\n",
      "Test size : 880\n",
      "\n",
      "Train label distribution:\n",
      "0    753\n",
      "1    836\n",
      "2    838\n",
      "3    834\n",
      "4    840\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Val label distribution:\n",
      "0    162\n",
      "1    179\n",
      "2    179\n",
      "3    179\n",
      "4    180\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test label distribution:\n",
      "0    161\n",
      "1    180\n",
      "2    180\n",
      "3    179\n",
      "4    180\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# stratified_splits() definition + run\n",
    "# ============================================\n",
    "\n",
    "def stratified_splits(\n",
    "    df: pd.DataFrame,\n",
    "    text_col: str = \"Description\",\n",
    "    label_col: str = \"Label\",\n",
    "    train_size: float = 0.70,\n",
    "    val_size: float = 0.15,\n",
    "    test_size: float = 0.15,\n",
    "    random_state: int = 42,\n",
    ") -> Tuple[pd.Series, pd.Series, pd.Series, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create stratified train/val/test splits.\n",
    "\n",
    "    Returns:\n",
    "        X_train, X_val, X_test, y_train, y_val, y_test\n",
    "    \"\"\"\n",
    "    assert np.isclose(train_size + val_size + test_size, 1.0), \\\n",
    "        \"train_size + val_size + test_size must equal 1.0\"\n",
    "\n",
    "    # Encode labels to ids\n",
    "    y_all = encode_labels(df[label_col])\n",
    "    X_all = df[text_col].values\n",
    "\n",
    "    # First split: train vs temp\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "        X_all,\n",
    "        y_all,\n",
    "        test_size=(1.0 - train_size),\n",
    "        stratify=y_all,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    # Second split: val vs test from temp\n",
    "    relative_test_size = test_size / (test_size + val_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(\n",
    "        X_temp,\n",
    "        y_temp,\n",
    "        test_size=relative_test_size,\n",
    "        stratify=y_temp,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = stratified_splits(df)\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Val size  :\", len(X_val))\n",
    "print(\"Test size :\", len(X_test))\n",
    "\n",
    "# Check label balance in each split\n",
    "print(\"\\nTrain label distribution:\")\n",
    "print(pd.Series(y_train).value_counts().sort_index())\n",
    "print(\"\\nVal label distribution:\")\n",
    "print(pd.Series(y_val).value_counts().sort_index())\n",
    "print(\"\\nTest label distribution:\")\n",
    "print(pd.Series(y_test).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebb9b6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded train shape: (4101, 128)\n",
      "Padded val shape  : (879, 128)\n",
      "Padded test shape : (880, 128)\n",
      "\n",
      "Example text: এই নাস্তিকের বাচ্চা নাস্তিক জায়স সন্তান তুই কেনো বললি যে তুই পরকাল বিস্বাস করস না তসলিমা তসলিমি নাসরিন যেমন দেশ থেকে বিতারিত করা হয়েছিল তোকেও তেমন এ দেশ থেকে বিতারিত করবো তুই তো একটা বেস্য মাগি তর মুখ দিয়ে তো এই কথাই সোভা পায় কারন মাগি রা আল্লাহ বিস্বাস করে না তুইও বাজারের মাগি\n",
      "Example sequence: [   5  305  114   78 5835  311   16  566 1823   13   16  109 1323  960\n",
      "    2  686 5836  803  542   70   14  543   31 2239 5837  746   71   70\n",
      "   14  543]\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Keras tokenizer + padded sequences\n",
    "# ============================================\n",
    "\n",
    "def build_keras_tokenizer(\n",
    "    texts: List[str],\n",
    "    num_words: int = None,\n",
    "    oov_token: str = \"[OOV]\"\n",
    ") -> Tokenizer:\n",
    "    \"\"\"\n",
    "    Fit a Keras Tokenizer on the training texts.\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def texts_to_padded_sequences(\n",
    "    tokenizer: Tokenizer,\n",
    "    texts: List[str],\n",
    "    max_len: int = 128\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a list/array of texts into padded sequences.\n",
    "    \"\"\"\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n",
    "    return padded\n",
    "\n",
    "\n",
    "max_len = 128\n",
    "tokenizer = build_keras_tokenizer(X_train)\n",
    "\n",
    "X_train_pad = texts_to_padded_sequences(tokenizer, X_train, max_len=max_len)\n",
    "X_val_pad = texts_to_padded_sequences(tokenizer, X_val, max_len=max_len)\n",
    "X_test_pad = texts_to_padded_sequences(tokenizer, X_test, max_len=max_len)\n",
    "\n",
    "print(\"Padded train shape:\", X_train_pad.shape)\n",
    "print(\"Padded val shape  :\", X_val_pad.shape)\n",
    "print(\"Padded test shape :\", X_test_pad.shape)\n",
    "\n",
    "# Look at one example\n",
    "idx = 0\n",
    "print(\"\\nExample text:\", X_train[idx])\n",
    "print(\"Example sequence:\", X_train_pad[idx][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dde80a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input_ids shape    : (8, 128)\n",
      "Sample attention_mask shape: (8, 128)\n",
      "\n",
      "First row input_ids (first 20 tokens):\n",
      "[     0   6386   4480 212478  75011   7802 240873   4691   4480  34619\n",
      "   9976  30414   2801   3458 147345      6  57158   2730  65828   9445]\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Transformer tokenizer helpers + test\n",
    "# ============================================\n",
    "\n",
    "def load_transformer_tokenizer(model_name: str = \"xlm-roberta-base\"):\n",
    "    \"\"\"\n",
    "    Load a pretrained Hugging Face tokenizer.\n",
    "\n",
    "    Based on EDA (Bangla + code-mixing), a multilingual subword model such as\n",
    "    'xlm-roberta-base' or 'bert-base-multilingual-cased' is appropriate.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def encode_texts_transformer(\n",
    "    tokenizer,\n",
    "    texts: List[str],\n",
    "    max_len: int = 128\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Encode texts using a Hugging Face tokenizer.\n",
    "\n",
    "    Returns numpy arrays:\n",
    "        {\n",
    "            \"input_ids\": shape (N, max_len),\n",
    "            \"attention_mask\": shape (N, max_len)\n",
    "        }\n",
    "    \"\"\"\n",
    "    encodings = tokenizer(\n",
    "        list(texts),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_len,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "\n",
    "    input_ids = np.array(encodings[\"input_ids\"], dtype=np.int64)\n",
    "    attention_mask = np.array(encodings[\"attention_mask\"], dtype=np.int64)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "\n",
    "# Load tokenizer and encode a small subset to test (to save time)\n",
    "hf_model_name = \"xlm-roberta-base\"\n",
    "hf_tokenizer = load_transformer_tokenizer(hf_model_name)\n",
    "\n",
    "sample_texts = list(X_train[:8])\n",
    "enc_sample = encode_texts_transformer(hf_tokenizer, sample_texts, max_len=128)\n",
    "\n",
    "print(\"Sample input_ids shape    :\", enc_sample[\"input_ids\"].shape)\n",
    "print(\"Sample attention_mask shape:\", enc_sample[\"attention_mask\"].shape)\n",
    "print(\"\\nFirst row input_ids (first 20 tokens):\")\n",
    "print(enc_sample[\"input_ids\"][0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e65e96fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary ===\n",
      "Final prepared df shape: (5860, 2)\n",
      "Train/Val/Test sizes: 4101 879 880\n",
      "Keras padded train shape: (4101, 128)\n",
      "Transformer sample Enc shape: (8, 128)\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# wrap up / summary print\n",
    "# ============================================\n",
    "\n",
    "print(\"=== Summary ===\")\n",
    "print(\"Final prepared df shape:\", df.shape)\n",
    "print(\"Train/Val/Test sizes:\", len(X_train), len(X_val), len(X_test))\n",
    "print(\"Keras padded train shape:\", X_train_pad.shape)\n",
    "print(\"Transformer sample Enc shape:\", enc_sample[\"input_ids\"].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
